{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "different-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import re\n",
    "from importlib import reload\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "import string\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "plt.style.use('fivethirtyeight')\n",
    "mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708480a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "documented-violence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS.update([\"a\",\"will\",\"el\",\"del\",\"lo\", \"abord\", \"absolument\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ailleurs\", \"ainsi\", \"ait\", \"allaient\", \"allo\",\n",
    " \"allons\", \"all√¥\", \"alors\", \"anterieur\", \"anterieure\", \"anterieures\", \"apres\", \"apr√®s\", \"as\", \"assez\", \"attendu\", \"au\",\n",
    " \"aucun\", \"aucune\", \"aucuns\", \"aujourd\", \"aujourd'hui\", \"aupres\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\",\n",
    " \"aurait\", \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autant\", \"autre\", \"autrefois\",\n",
    " \"autrement\", \"autres\", \"autrui\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\",\n",
    " \"aviez\", \"avions\", \"avoir\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"b\", \"bah\", \"bas\", \"basee\", \"bat\", \"beau\", \"beaucoup\",\n",
    " \"bien\", \"bigre\", \"bon\", \"boum\", \"bravo\", \"brrr\", \"c\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\", \"celle-ci\", \"celle-l√†\",\n",
    " \"celles\", \"celles-ci\", \"celles-l√†\", \"celui\", \"celui-ci\", \"celui-l√†\", \"cel√†\", \"cent\", \"cependant\", \"certain\",\n",
    " \"certaine\", \"certaines\", \"certains\", \"certes\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-l√†\", \"chacun\",\n",
    " \"chacune\", \"chaque\", \"cher\", \"chers\", \"chez\", \"chiche\", \"chut\", \"ch√®re\", \"ch√®res\", \"ci\", \"cinq\", \"cinquantaine\",\n",
    " \"cinquante\", \"cinquanti√®me\", \"cinqui√®me\", \"clac\", \"clic\", \"combien\", \"comme\", \"comment\", \"comparable\", \"comparables\",\n",
    " \"compris\", \"concernant\", \"contre\", \"couic\", \"crac\", \"d\", \"da\", \"dans\", \"de\", \"debout\", \"dedans\", \"dehors\", \"deja\",\n",
    " \"del√†\", \"depuis\", \"dernier\", \"derniere\", \"derriere\", \"derri√®re\", \"des\", \"desormais\", \"desquelles\", \"desquels\",\n",
    " \"dessous\", \"dessus\", \"deux\", \"deuxi√®me\", \"deuxi√®mement\", \"devant\", \"devers\", \"devra\", \"devrait\", \"different\",\n",
    " \"differentes\", \"differents\", \"diff√©rent\", \"diff√©rente\", \"diff√©rentes\", \"diff√©rents\", \"dire\", \"directe\", \"directement\",\n",
    " \"dit\", \"dite\", \"dits\", \"divers\", \"diverse\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dixi√®me\", \"doit\",\n",
    " \"doivent\", \"donc\", \"dont\", \"dos\", \"douze\", \"douzi√®me\", \"dring\", \"droite\", \"du\", \"duquel\", \"durant\", \"d√®s\", \"d√©but\",\n",
    " \"d√©sormais\", \"e\", \"effet\", \"egale\", \"egalement\", \"egales\", \"eh\", \"elle\", \"elle-m√™me\", \"elles\", \"elles-m√™mes\", \"en\",\n",
    " \"encore\", \"enfin\", \"entre\", \"envers\", \"environ\", \"es\", \"essai\", \"est\", \"et\", \"etant\", \"etc\", \"etre\", \"eu\", \"eue\",\n",
    " \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eux-m√™mes\",\n",
    " \"exactement\", \"except√©\", \"extenso\", \"exterieur\", \"e√ªmes\", \"e√ªt\", \"e√ªtes\", \"f\", \"fais\", \"faisaient\", \"faisant\", \"fait\",\n",
    " \"faites\", \"fa√ßon\", \"feront\", \"fi\", \"flac\", \"floc\", \"fois\", \"font\", \"force\", \"furent\", \"fus\", \"fusse\", \"fussent\",\n",
    " \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"f√ªmes\", \"f√ªt\", \"f√ªtes\", \"g\", \"gens\", \"h\", \"ha\", \"haut\", \"hein\", \"hem\", \"hep\",\n",
    " \"hi\", \"ho\", \"hol√†\", \"hop\", \"hormis\", \"hors\", \"hou\", \"houp\", \"hue\", \"hui\", \"huit\", \"huiti√®me\", \"hum\", \"hurrah\", \"h√©\",\n",
    " \"h√©las\", \"i\", \"ici\", \"il\", \"ils\", \"importe\", \"j\", \"je\", \"jusqu\", \"jusque\", \"juste\", \"k\", \"l\", \"la\", \"laisser\",\n",
    " \"laquelle\", \"las\", \"le\", \"lequel\", \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"longtemps\", \"lors\", \"lorsque\",\n",
    " \"lui\", \"lui-meme\", \"lui-m√™me\", \"l√†\", \"l√®s\", \"m\", \"ma\", \"maint\", \"maintenant\", \"mais\", \"malgre\", \"malgr√©\", \"maximale\",\n",
    " \"me\", \"meme\", \"memes\", \"merci\", \"mes\", \"mien\", \"mienne\", \"miennes\", \"miens\", \"mille\", \"mince\", \"mine\", \"minimale\",\n",
    " \"moi\", \"moi-meme\", \"moi-m√™me\", \"moindres\", \"moins\", \"mon\", \"mot\", \"moyennant\", \"multiple\", \"multiples\", \"m√™me\",\n",
    " \"m√™mes\", \"n\", \"na\", \"naturel\", \"naturelle\", \"naturelles\", \"ne\", \"neanmoins\", \"necessaire\", \"necessairement\", \"neuf\",\n",
    " \"neuvi√®me\", \"ni\", \"nombreuses\", \"nombreux\", \"nomm√©s\", \"non\", \"nos\", \"notamment\", \"notre\", \"nous\", \"nous-m√™mes\",\n",
    " \"nouveau\", \"nouveaux\", \"nul\", \"n√©anmoins\", \"n√¥tre\", \"n√¥tres\", \"o\", \"oh\", \"oh√©\", \"oll√©\", \"ol√©\", \"on\", \"ont\", \"onze\",\n",
    " \"onzi√®me\", \"ore\", \"ou\", \"ouf\", \"ouias\", \"oust\", \"ouste\", \"outre\", \"ouvert\", \"ouverte\", \"ouverts\", \"o|\", \"o√π\", \"p\",\n",
    " \"paf\", \"pan\", \"par\", \"parce\", \"parfois\", \"parle\", \"parlent\", \"parler\", \"parmi\", \"parole\", \"parseme\", \"partant\",\n",
    " \"particulier\", \"particuli√®re\", \"particuli√®rement\", \"pas\", \"pass√©\", \"pendant\", \"pense\", \"permet\", \"personne\",\n",
    " \"personnes\", \"peu\", \"peut\", \"peuvent\", \"peux\", \"pff\", \"pfft\", \"pfut\", \"pif\", \"pire\", \"pi√®ce\", \"plein\", \"plouf\",\n",
    " \"plupart\", \"plus\", \"plusieurs\", \"plut√¥t\", \"possessif\", \"possessifs\", \"possible\", \"possibles\", \"pouah\", \"pour\",\n",
    " \"pourquoi\", \"pourrais\", \"pourrait\", \"pouvait\", \"prealable\", \"precisement\", \"premier\", \"premi√®re\", \"premi√®rement\",\n",
    " \"pres\", \"probable\", \"probante\", \"procedant\", \"proche\", \"pr√®s\", \"psitt\", \"pu\", \"puis\", \"puisque\", \"pur\", \"pure\", \"q\",\n",
    " \"qu\", \"quand\", \"quant\", \"quant-√†-soi\", \"quanta\", \"quarante\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatri√®me\",\n",
    " \"quatri√®mement\", \"que\", \"quel\", \"quelconque\", \"quelle\", \"quelles\", \"quelqu'un\", \"quelque\", \"quelques\", \"quels\", \"qui\",\n",
    " \"quiconque\", \"quinze\", \"quoi\", \"quoique\", \"r\", \"rare\", \"rarement\", \"rares\", \"relative\", \"relativement\", \"remarquable\",\n",
    " \"rend\", \"rendre\", \"restant\", \"reste\", \"restent\", \"restrictif\", \"retour\", \"revoici\", \"revoil√†\", \"rien\", \"s\", \"sa\",\n",
    " \"sacrebleu\", \"sait\", \"sans\", \"sapristi\", \"sauf\", \"se\", \"sein\", \"seize\", \"selon\", \"semblable\", \"semblaient\", \"semble\",\n",
    " \"semblent\", \"sent\", \"sept\", \"septi√®me\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\",\n",
    " \"serions\", \"serons\", \"seront\", \"ses\", \"seul\", \"seule\", \"seulement\", \"si\", \"sien\", \"sienne\", \"siennes\", \"siens\",\n",
    " \"sinon\", \"six\", \"sixi√®me\", \"soi\", \"soi-m√™me\", \"soient\", \"sois\", \"soit\", \"soixante\", \"sommes\", \"son\", \"sont\", \"sous\",\n",
    " \"souvent\", \"soyez\", \"soyons\", \"specifique\", \"specifiques\", \"speculatif\", \"stop\", \"strictement\", \"subtiles\",\n",
    " \"suffisant\", \"suffisante\", \"suffit\", \"suis\", \"suit\", \"suivant\", \"suivante\", \"suivantes\", \"suivants\", \"suivre\", \"sujet\",\n",
    " \"superpose\", \"sur\", \"surtout\", \"t\", \"ta\", \"tac\", \"tandis\", \"tant\", \"tardive\", \"te\", \"tel\", \"telle\", \"tellement\",\n",
    " \"telles\", \"tels\", \"tenant\", \"tend\", \"tenir\", \"tente\", \"tes\", \"tic\", \"tien\", \"tienne\", \"tiennes\", \"tiens\", \"toc\", \"toi\",\n",
    " \"toi-m√™me\", \"ton\", \"touchant\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"tres\",\n",
    " \"trois\", \"troisi√®me\", \"troisi√®mement\", \"trop\", \"tr√®s\", \"tsoin\", \"tsouin\", \"tu\", \"t√©\", \"u\", \"un\", \"une\", \"unes\",\n",
    " \"uniformement\", \"unique\", \"uniques\", \"uns\", \"v\", \"va\", \"vais\", \"valeur\", \"vas\", \"vers\", \"via\", \"vif\", \"vifs\", \"vingt\",\n",
    " \"vivat\", \"vive\", \"vives\", \"vlan\", \"voici\", \"voie\", \"voient\", \"voil√†\", \"voire\", \"vont\", \"vos\", \"votre\", \"vous\",\n",
    " \"vous-m√™mes\", \"vu\", \"v√©\", \"v√¥tre\", \"v√¥tres\", \"w\", \"x\", \"y\", \"z\", \"zut\", \"√†\", \"√¢\", \"√ßa\", \"√®s\", \"√©taient\", \"√©tais\",\n",
    " \"√©tait\", \"√©tant\", \"√©tat\", \"√©tiez\", \"√©tions\", \"√©t√©\", \"√©t√©e\", \"√©t√©es\", \"√©t√©s\", \"√™tes\", \"√™tre\", \"√¥\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consistent-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arabic-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_tweets_dataframe as cld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distinguished-denver",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(cld);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "manufactured-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=pd.read_excel(\"week3_new.xlsx\",engine='openpyxl',dtype={'tweet_id':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "buried-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner=cld.CleanTweets(df_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-fantasy",
   "metadata": {},
   "source": [
    "**Using cleaner module from clean_tweets_dataframe to clean the imported dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supported-clarity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12678, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hindu-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.dropna(subset=['cleaned_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extended-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=cleaner.drop_unwanted_column(df_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collect-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=cleaner.convert_to_datetime(df_tweet)\n",
    "df_tweet=cleaner.convert_to_numbers(df_tweet)\n",
    "df_tweet=cleaner.treat_special_characters(df_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "above-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=cleaner.remove_other_languages_tweets(df_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "connected-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=cleaner.drop_retweets(df_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "seven-boost",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7749, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lang</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>...</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-09 07:03:39+00:00</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>Polar bear killed after hurting French tourist...</td>\n",
       "      <td>Polar bear killed after hurting French tourist...</td>\n",
       "      <td>-0.094444</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>Negative</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4942</td>\n",
       "      <td>398</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://twitter.com/Noticieros_MEX/status/1556...</td>\n",
       "      <td>1556898991434072064</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-09 07:03:39+00:00</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>South Korea: 7 die from torrential rain in Seo...</td>\n",
       "      <td>South Korea: 7 die from torrential rain in Seoul</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4942</td>\n",
       "      <td>398</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://twitter.com/Noticieros_MEX/status/1556...</td>\n",
       "      <td>1556898990251249665</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-09 07:03:39+00:00</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>Donald Trump says FBI carried out 'unannounced...</td>\n",
       "      <td>Donald Trump says FBI carried out 'unannounced...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4942</td>\n",
       "      <td>398</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://twitter.com/Noticieros_MEX/status/1556...</td>\n",
       "      <td>1556898988544200704</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-09 07:03:39+00:00</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>Ahmaud Arbery's killers get life sentences for...</td>\n",
       "      <td>Ahmaud Arbery's killers get life sentences for...</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Negative</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4942</td>\n",
       "      <td>398</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://twitter.com/Noticieros_MEX/status/1556...</td>\n",
       "      <td>1556898987373957121</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-09 07:03:38+00:00</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>Cuba: Massive fire at oil storage facility eng...</td>\n",
       "      <td>Cuba: Massive fire at oil storage facility eng...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4942</td>\n",
       "      <td>398</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>https://twitter.com/Noticieros_MEX/status/1556...</td>\n",
       "      <td>1556898986187005952</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12672</th>\n",
       "      <td>2022-08-02 07:22:37+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...</td>\n",
       "      <td>üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>2328</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Airdrop, Airdrops, Crypto</td>\n",
       "      <td>CryptoTownEU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/MeghAhmed11/status/1554367...</td>\n",
       "      <td>1554367048322408449</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12674</th>\n",
       "      <td>2022-08-02 07:17:52+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...</td>\n",
       "      <td>üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>956</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Airdrop, Airdrops, Crypto</td>\n",
       "      <td>CryptoTownEU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/AfandiRastum/status/155436...</td>\n",
       "      <td>1554365851700428800</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12675</th>\n",
       "      <td>2022-08-02 07:17:40+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...</td>\n",
       "      <td>üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Airdrop, Airdrops, Crypto</td>\n",
       "      <td>CryptoTownEU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/FarhanMoradpour/status/155...</td>\n",
       "      <td>1554365801641172993</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12676</th>\n",
       "      <td>2022-08-02 07:16:00+00:00</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>-The Best Opportunity To Earn Crypto Via Readi...</td>\n",
       "      <td>-The Best Opportunity To Earn Crypto Via Readi...</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>ReadFi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/hoailinhy59/status/1554365...</td>\n",
       "      <td>1554365382433165312</td>\n",
       "      <td>Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12677</th>\n",
       "      <td>2022-08-02 07:11:55+00:00</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...</td>\n",
       "      <td>üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>780</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Airdrop, Airdrops, Crypto</td>\n",
       "      <td>CryptoTownEU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://twitter.com/Sars2000340/status/1554364...</td>\n",
       "      <td>1554364354711195648</td>\n",
       "      <td>Retweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7749 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     created_at               source  \\\n",
       "0     2022-08-09 07:03:39+00:00                IFTTT   \n",
       "1     2022-08-09 07:03:39+00:00                IFTTT   \n",
       "2     2022-08-09 07:03:39+00:00                IFTTT   \n",
       "3     2022-08-09 07:03:39+00:00                IFTTT   \n",
       "4     2022-08-09 07:03:38+00:00                IFTTT   \n",
       "...                         ...                  ...   \n",
       "12672 2022-08-02 07:22:37+00:00  Twitter for Android   \n",
       "12674 2022-08-02 07:17:52+00:00      Twitter Web App   \n",
       "12675 2022-08-02 07:17:40+00:00  Twitter for Android   \n",
       "12676 2022-08-02 07:16:00+00:00      Twitter Web App   \n",
       "12677 2022-08-02 07:11:55+00:00  Twitter for Android   \n",
       "\n",
       "                                           original_text  \\\n",
       "0      Polar bear killed after hurting French tourist...   \n",
       "1      South Korea: 7 die from torrential rain in Seo...   \n",
       "2      Donald Trump says FBI carried out 'unannounced...   \n",
       "3      Ahmaud Arbery's killers get life sentences for...   \n",
       "4      Cuba: Massive fire at oil storage facility eng...   \n",
       "...                                                  ...   \n",
       "12672  RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...   \n",
       "12674  RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...   \n",
       "12675  RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...   \n",
       "12676  -The Best Opportunity To Earn Crypto Via Readi...   \n",
       "12677  RT @CryptoTownEU: üöÄ Airdrop: ReadFi\\nüí∞ Value: ...   \n",
       "\n",
       "                                            cleaned_text  polarity  \\\n",
       "0      Polar bear killed after hurting French tourist... -0.094444   \n",
       "1      South Korea: 7 die from torrential rain in Seoul   0.000000   \n",
       "2      Donald Trump says FBI carried out 'unannounced...  0.000000   \n",
       "3      Ahmaud Arbery's killers get life sentences for... -0.800000   \n",
       "4      Cuba: Massive fire at oil storage facility eng...  0.000000   \n",
       "...                                                  ...       ...   \n",
       "12672  üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...  0.000000   \n",
       "12674  üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...  0.000000   \n",
       "12675  üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...  0.000000   \n",
       "12676  -The Best Opportunity To Earn Crypto Via Readi...  0.683333   \n",
       "12677  üöÄ Airdrop: ReadFi\\nüí∞ Value: 20 $RDF ($1.2)\\nüë• ...  0.000000   \n",
       "\n",
       "       subjectivity sentiment lang  likes_count  reply_count  ...  \\\n",
       "0          0.083333  Negative   en            0            0  ...   \n",
       "1          0.000000   Neutral   en            0            0  ...   \n",
       "2          0.000000   Neutral   en            0            0  ...   \n",
       "3          0.900000  Negative   en            0            0  ...   \n",
       "4          0.500000   Neutral   en            0            0  ...   \n",
       "...             ...       ...  ...          ...          ...  ...   \n",
       "12672      0.000000   Neutral   en            0            0  ...   \n",
       "12674      0.000000   Neutral   en            0            0  ...   \n",
       "12675      0.000000   Neutral   en            0            0  ...   \n",
       "12676      0.450000  Positive   en            0            0  ...   \n",
       "12677      0.000000   Neutral   en            0            0  ...   \n",
       "\n",
       "       followers_count friends_count  possibly_sensitive  hashtags  \\\n",
       "0                 4942           398               False       NaN   \n",
       "1                 4942           398               False       NaN   \n",
       "2                 4942           398               False       NaN   \n",
       "3                 4942           398               False       NaN   \n",
       "4                 4942           398               False       NaN   \n",
       "...                ...           ...                 ...       ...   \n",
       "12672               18          2328               False       NaN   \n",
       "12674               22           956               False       NaN   \n",
       "12675                1            30               False       NaN   \n",
       "12676                5            57               False    ReadFi   \n",
       "12677               64           780               False       NaN   \n",
       "\n",
       "                retweet_hashtags user_mentions   place  \\\n",
       "0                            NaN           NaN  Mexico   \n",
       "1                            NaN           NaN  Mexico   \n",
       "2                            NaN           NaN  Mexico   \n",
       "3                            NaN           NaN  Mexico   \n",
       "4                            NaN           NaN  Mexico   \n",
       "...                          ...           ...     ...   \n",
       "12672  Airdrop, Airdrops, Crypto  CryptoTownEU     NaN   \n",
       "12674  Airdrop, Airdrops, Crypto  CryptoTownEU     NaN   \n",
       "12675  Airdrop, Airdrops, Crypto  CryptoTownEU     NaN   \n",
       "12676                        NaN           NaN     NaN   \n",
       "12677  Airdrop, Airdrops, Crypto  CryptoTownEU     NaN   \n",
       "\n",
       "                                               tweet_url             tweet_id  \\\n",
       "0      https://twitter.com/Noticieros_MEX/status/1556...  1556898991434072064   \n",
       "1      https://twitter.com/Noticieros_MEX/status/1556...  1556898990251249665   \n",
       "2      https://twitter.com/Noticieros_MEX/status/1556...  1556898988544200704   \n",
       "3      https://twitter.com/Noticieros_MEX/status/1556...  1556898987373957121   \n",
       "4      https://twitter.com/Noticieros_MEX/status/1556...  1556898986187005952   \n",
       "...                                                  ...                  ...   \n",
       "12672  https://twitter.com/MeghAhmed11/status/1554367...  1554367048322408449   \n",
       "12674  https://twitter.com/AfandiRastum/status/155436...  1554365851700428800   \n",
       "12675  https://twitter.com/FarhanMoradpour/status/155...  1554365801641172993   \n",
       "12676  https://twitter.com/hoailinhy59/status/1554365...  1554365382433165312   \n",
       "12677  https://twitter.com/Sars2000340/status/1554364...  1554364354711195648   \n",
       "\n",
       "      tweet_category  \n",
       "0              Tweet  \n",
       "1              Tweet  \n",
       "2              Tweet  \n",
       "3              Tweet  \n",
       "4              Tweet  \n",
       "...              ...  \n",
       "12672        Retweet  \n",
       "12674        Retweet  \n",
       "12675        Retweet  \n",
       "12676          Tweet  \n",
       "12677        Retweet  \n",
       "\n",
       "[7749 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_tweet.shape)\n",
    "df_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "applied-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = df_tweet[df_tweet.original_author != 'dwnews']\n",
    "df_tweet = df_tweet[df_tweet.original_author != '123_INFO_DE']\n",
    "df_tweet = df_tweet[df_tweet.original_author != 'rogue_corq']\n",
    "df_tweet = df_tweet[df_tweet.original_author != 'Noticieros_MEX']\n",
    "df_tweet = df_tweet[df_tweet.original_author != 'EUwatchers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "second-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop crypo bots \n",
    "\n",
    "# fo week2\n",
    "\n",
    "# df_tweet=df_tweet[~pd.Series(df_tweet['cleaned_text']).str.contains(\"The Best Opportunity To Earn Crypto Via Reading\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "robust-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for week 3\n",
    "df_tweet=df_tweet[~pd.Series(df_tweet['cleaned_text']).str.contains(\"The Best Opportunity To Earn Crypto Via Reading\")]\n",
    "df_tweet=df_tweet[~pd.Series(df_tweet['original_text']).str.contains(\"@CryptoTownEU\")]\n",
    "df_tweet=df_tweet[~pd.Series(df_tweet['original_text']).str.contains(\"Airdrop\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pending-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "available-element",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en      2346\n",
       "fr      1803\n",
       "kiny     150\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-colombia",
   "metadata": {},
   "source": [
    "###  Export french tweets, make translation and insert back new translated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hispanic-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans=df_tweet.query(\"lang=='fr'| lang =='kiny'\")[['original_text','cleaned_text','lang']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-february",
   "metadata": {},
   "source": [
    "### Translation with Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "noticed-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "charitable-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = pygsheets.authorize(service_file='tweet-auto-01-833e318c05c8.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fatty-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet = gc.open('make_trans')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "breeding-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet.set_dataframe(df_trans,start='A1',copy_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "victorian-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet.update_value('E1','translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mounted-humanity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.google.com/spreadsheets/d/1ITYeneA29Vvk_wKESJh5J92reOKcyjDQNo_yitMXPGc/edit#gid=0\n"
     ]
    }
   ],
   "source": [
    "# Browse the doc and make translations manually\n",
    "print(sheet.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-posting",
   "metadata": {},
   "source": [
    "#### loop to translate, slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2,len(df_trans)+2):\n",
    "#     sheet.update_value('E'+str(i),'=GOOGLETRANSLATE(C'+str(i)+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04c84d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_to_translate=df_tweet.query(\"lang=='fr'| lang =='kiny'\")['cleaned_text'].to_dict()\n",
    "\n",
    "# dict_translated=sheet.get_as_df(index_column=1)[['translation']].to_dict()['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd38f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_translate=df_tweet.query(\"lang=='fr'| lang =='kiny'\")['cleaned_text'].to_dict()\n",
    "\n",
    "df_list=sheet.get_all_values(include_tailing_empty=False,include_tailing_empty_rows=False)\n",
    "\n",
    "dict_translated=pd.DataFrame(df_list[1:],columns=df_list[0]).set_index('')[['translation']].to_dict()['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "crucial-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "thirty-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in dict_to_translate.keys():\n",
    "    df_tweet.loc[idx, 'cleaned_text'] = dict_translated[str(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in dict_translated.keys():\n",
    "    blob = TextBlob(clean(dict_translated[idx], no_emoji=True))\n",
    "    pol = blob.sentiment.polarity\n",
    "    df_tweet.loc[idx, 'sentiment'] = 'Positive' if pol > 0 else (\n",
    "        'Negative' if pol < 0 else 'Neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-reset",
   "metadata": {},
   "source": [
    "#### Save new dataframe for dashboard creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['created_at'] = df_tweet['created_at'].apply(lambda x: x.replace(tzinfo=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.to_excel('plotly_dashboard/week3_processed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-sharp",
   "metadata": {},
   "source": [
    "**Now we continue with Tweet and Replies Only! Excluding Retweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=df_tweet.query(\"tweet_category=='Tweet' or tweet_category== 'Reply'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-foundation",
   "metadata": {},
   "source": [
    "### EDA of Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentage of missing values in every column\n",
    "ax = df_tweet.isna().sum().sort_values().plot(kind = 'barh', figsize = (7, 9))\n",
    "plt.title('Percentage of Missing Values Per Column in Tweets data', fontdict={'size':15})\n",
    "\n",
    "for p in ax.patches:\n",
    "    percentage ='{:,.3f}%'.format((p.get_width()/df_tweet.shape[0])*100)\n",
    "    width, height =p.get_width(),p.get_height()\n",
    "    x=p.get_x()+width+0.02\n",
    "    y=p.get_y()+height/2\n",
    "    ax.annotate(percentage,(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['place'].value_counts(sort=True, ascending=False)[:10].plot(kind='barh',\n",
    "                                                                     figsize=(12,5),xlabel='Place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_date=df_tweet.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_date.resample('D').mean()[['polarity','subjectivity']].dropna().plot(figsize=(10,6),xlabel='Date,time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-grenada",
   "metadata": {},
   "source": [
    "### uni-variate Analysis on Hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_df=df_tweet[['original_text','hashtags','retweet_hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['original_text']=df_tweet['original_text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hashtags(df_tweet):\n",
    "    '''This function will extract hashtags'''\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', df_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-cylinder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtag_df['hashtag_check']=df_tweet.original_text.apply(find_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_df.dropna(subset=['hashtag_check'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list=list(hashtag_df['hashtag_check'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_list_df = pd.DataFrame([tag for tags_row in tags_list for tag in tags_row],columns=['hashtag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=hashtags_list_df.value_counts()[:10].plot(kind='bar',figsize=(12,5),xlabel='Actual Hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert hastags to lowercase\n",
    "hashtags_list_df['hashtag'] = hashtags_list_df['hashtag'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=hashtags_list_df.value_counts()[:10].plot(kind='bar',figsize=(12,5),xlabel='Hashtags in Lowercase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_summary=df_tweet.groupby('original_author').agg({'cleaned_text':'count','followers_count':'max',\n",
    "                                         'polarity':'mean','subjectivity':'mean', 'sentiment':pd.Series.mode})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_summary.sort_values(by='cleaned_text',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-bridal",
   "metadata": {},
   "source": [
    "Sentiment summary of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_grouped = df_tweet.groupby('sentiment').count()['cleaned_text'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sentiment', data=df_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-blair",
   "metadata": {},
   "source": [
    "**Most frequent words in our tweets dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tweets=df_tweet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text Preprocessing\n",
    "english_tweets['cleaned_text'] = english_tweets['cleaned_text'].apply(\n",
    "    lambda x: clean(x,\n",
    "                    no_emoji=True,\n",
    "                    lower=True,\n",
    "                    no_punct=True,\n",
    "                    no_line_breaks=True,\n",
    "                    no_currency_symbols=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=' '.join(english_tweets.cleaned_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_obj=WordCloud(width=1000,height=600,stopwords=STOPWORDS).generate(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-background",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "fgg=plt.imshow(wordcloud_obj)\n",
    "plt.axis('off')\n",
    "# plt.title('Most Frequent Words In Our Tweets',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgg.figure.savefig('plotly_dashboard/cw_rdf_week3.png',bbox_inches='tight',pad_inches=0)\n",
    "# fgg.figure.savefig('plotly_dashboard/assets/cw_rdf_week3.png',bbox_inches='tight',pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-macro",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tweets_df):\n",
    "    # Converting tweets to list of words For feature engineering\n",
    "    sentence_list = [tweet for tweet in tweets_df['cleaned_text']]\n",
    "    word_list = [sent.split() for sent in sentence_list]\n",
    "    #Save only words and excludes emojis, punctuations\n",
    "    word_list_new=[]\n",
    "    for sent in word_list:\n",
    "        word_list_new.append([re.split(r'\\W+',word) for word in sent if word not in STOPWORDS and not word.isdigit()])\n",
    "    \n",
    "    word_list_final=[]\n",
    "    for sent in word_list_new:\n",
    "        word_list_final.append([i[0] for i in sent])\n",
    "    # Create dictionary which contains Id and word \n",
    "    word_to_id = corpora.Dictionary(word_list_final)\n",
    "    corpus_1 = [word_to_id.doc2bow(tweet) for tweet in word_list_final]\n",
    "\n",
    "    return word_list_final, word_to_id, corpus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, id2word, corpus=preprocess_data(english_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-greensboro",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-niger",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "\n",
    "#It's a measure of how good the model is. The lower the better. Perplexity is a negative value\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# doc_lda = lda_model[corpus]\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\n Ldamodel Coherence Score/Accuracy on Tweets: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-diamond",
   "metadata": {},
   "source": [
    "**Model 1 words with some digits, 5 topics**\n",
    "\n",
    "Perplexity:  -9.857909007134007\\\n",
    "Ldamodel Coherence Score/Accuracy on Tweets:  0.4242327533406264\n",
    "\n",
    "**Model 2 words without digits, 5 topics**\n",
    "\n",
    "Perplexity:  -9.82031321033761\\\n",
    "Ldamodel Coherence Score/Accuracy on Tweets:  0.4368570452021986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-cutting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-humor",
   "metadata": {},
   "source": [
    "###  END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-popularity",
   "metadata": {},
   "source": [
    "**Number of Topics optimatization**\n",
    "\n",
    "As we can see the coherence accuracy increases with number of topics which is expected but again \\\n",
    "many topics again would lead to meaningless conclusion. Let's use elbow method to find optimum number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find best LDA Model considering different number of topics\n",
    "\n",
    "# params_dic={'topics_number':[],'coherence':[],'perplexity':[]}\n",
    "# for top_number in range(1,10):\n",
    "#     lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "#                                             id2word=id2word,\n",
    "#                                             num_topics=top_number, \n",
    "#                                             random_state=100,\n",
    "#                                             update_every=1,\n",
    "#                                             chunksize=100,\n",
    "#                                             passes=10,\n",
    "#                                             alpha='auto',\n",
    "#                                             per_word_topics=False)\n",
    "    \n",
    "#     perplexity=lda_model.log_perplexity(corpus)  \n",
    "#     # Compute Coherence Score\n",
    "#     coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=id2word, coherence='c_v')\n",
    "#     coherence_lda = coherence_model_lda.get_coherence()\n",
    "#     params_dic['topics_number'].append(top_number)\n",
    "#     params_dic['coherence'].append(coherence_lda)\n",
    "#     params_dic['perplexity'].append(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-auckland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(params_dic).plot(y=['coherence'])\n",
    "# # pd.read_csv(\"elbow_metrics.csv\",index_col=0).plot(y=['coherence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas = list(np.arange(0.01, 3, 0.1))\n",
    "# alphas.append('symmetric')\n",
    "# alphas.append('asymmetric')\n",
    "# alphas.append('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find best LDA Model considering different number of alpha\n",
    "\n",
    "# params_dic_alpha={'alpha':[],'coherence':[],'perplexity':[]}\n",
    "# for alpha in alphas:\n",
    "#     lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "#                                             id2word=id2word,\n",
    "#                                             num_topics=7, \n",
    "#                                             random_state=100,\n",
    "#                                             update_every=1,\n",
    "#                                             chunksize=100,\n",
    "#                                             passes=10,\n",
    "#                                             alpha=alpha,\n",
    "#                                             per_word_topics=False)\n",
    "    \n",
    "#     perplexity=lda_model.log_perplexity(corpus)  \n",
    "#     # Compute Coherence Score\n",
    "#     coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=id2word, coherence='c_v')\n",
    "#     coherence_lda = coherence_model_lda.get_coherence()\n",
    "#     params_dic_alpha['alpha'].append(alpha)\n",
    "#     params_dic_alpha['coherence'].append(coherence_lda)\n",
    "#     params_dic_alpha['perplexity'].append(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(params_dic_alpha).plot(y='coherence')#.sort_values('coherence',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-territory",
   "metadata": {},
   "source": [
    "**Model with selected optimum parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build LDA model\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=7, \n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=100,\n",
    "#                                            passes=10,\n",
    "#                                            alpha=0.1,\n",
    "#                                            per_word_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute Perplexity\n",
    "# #It's a measure of how good the model is. The lower the better. Perplexity is a negative value\n",
    "# print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# # doc_lda = lda_model[corpus]\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=id2word, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\n Ldamodel Coherence Score/Accuracy on Tweets: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "# LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "# LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
